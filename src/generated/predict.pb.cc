// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: predict.proto

#include "predict.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/extension_set.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>

extern PROTOBUF_INTERNAL_EXPORT_model_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_ModelSpec_model_2eproto;
extern PROTOBUF_INTERNAL_EXPORT_predict_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_PredictRequest_InputsEntry_DoNotUse_predict_2eproto;
extern PROTOBUF_INTERNAL_EXPORT_predict_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_PredictResponse_OutputsEntry_DoNotUse_predict_2eproto;
extern PROTOBUF_INTERNAL_EXPORT_tensor_2eproto ::google::protobuf::internal::SCCInfo<2> scc_info_TensorProto_tensor_2eproto;
namespace tensorflow {
namespace serving {
class PredictRequest_InputsEntry_DoNotUseDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<PredictRequest_InputsEntry_DoNotUse> _instance;
} _PredictRequest_InputsEntry_DoNotUse_default_instance_;
class PredictRequestDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<PredictRequest> _instance;
} _PredictRequest_default_instance_;
class PredictResponse_OutputsEntry_DoNotUseDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<PredictResponse_OutputsEntry_DoNotUse> _instance;
} _PredictResponse_OutputsEntry_DoNotUse_default_instance_;
class PredictResponseDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<PredictResponse> _instance;
} _PredictResponse_default_instance_;
}  // namespace serving
}  // namespace tensorflow
static void InitDefaultsPredictRequest_InputsEntry_DoNotUse_predict_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::tensorflow::serving::_PredictRequest_InputsEntry_DoNotUse_default_instance_;
    new (ptr) ::tensorflow::serving::PredictRequest_InputsEntry_DoNotUse();
  }
  ::tensorflow::serving::PredictRequest_InputsEntry_DoNotUse::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_PredictRequest_InputsEntry_DoNotUse_predict_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsPredictRequest_InputsEntry_DoNotUse_predict_2eproto}, {
      &scc_info_TensorProto_tensor_2eproto.base,}};

static void InitDefaultsPredictRequest_predict_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::tensorflow::serving::_PredictRequest_default_instance_;
    new (ptr) ::tensorflow::serving::PredictRequest();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::tensorflow::serving::PredictRequest::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<2> scc_info_PredictRequest_predict_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 2, InitDefaultsPredictRequest_predict_2eproto}, {
      &scc_info_ModelSpec_model_2eproto.base,
      &scc_info_PredictRequest_InputsEntry_DoNotUse_predict_2eproto.base,}};

static void InitDefaultsPredictResponse_OutputsEntry_DoNotUse_predict_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::tensorflow::serving::_PredictResponse_OutputsEntry_DoNotUse_default_instance_;
    new (ptr) ::tensorflow::serving::PredictResponse_OutputsEntry_DoNotUse();
  }
  ::tensorflow::serving::PredictResponse_OutputsEntry_DoNotUse::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_PredictResponse_OutputsEntry_DoNotUse_predict_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsPredictResponse_OutputsEntry_DoNotUse_predict_2eproto}, {
      &scc_info_TensorProto_tensor_2eproto.base,}};

static void InitDefaultsPredictResponse_predict_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::tensorflow::serving::_PredictResponse_default_instance_;
    new (ptr) ::tensorflow::serving::PredictResponse();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::tensorflow::serving::PredictResponse::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<2> scc_info_PredictResponse_predict_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 2, InitDefaultsPredictResponse_predict_2eproto}, {
      &scc_info_ModelSpec_model_2eproto.base,
      &scc_info_PredictResponse_OutputsEntry_DoNotUse_predict_2eproto.base,}};

void InitDefaults_predict_2eproto() {
  ::google::protobuf::internal::InitSCC(&scc_info_PredictRequest_InputsEntry_DoNotUse_predict_2eproto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_PredictRequest_predict_2eproto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_PredictResponse_OutputsEntry_DoNotUse_predict_2eproto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_PredictResponse_predict_2eproto.base);
}

::google::protobuf::Metadata file_level_metadata_predict_2eproto[4];
constexpr ::google::protobuf::EnumDescriptor const** file_level_enum_descriptors_predict_2eproto = nullptr;
constexpr ::google::protobuf::ServiceDescriptor const** file_level_service_descriptors_predict_2eproto = nullptr;

const ::google::protobuf::uint32 TableStruct_predict_2eproto::offsets[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictRequest_InputsEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictRequest_InputsEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictRequest_InputsEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictRequest_InputsEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictRequest, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictRequest, model_spec_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictRequest, inputs_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictRequest, output_filter_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictResponse_OutputsEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictResponse_OutputsEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictResponse_OutputsEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictResponse_OutputsEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictResponse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictResponse, model_spec_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::PredictResponse, outputs_),
};
static const ::google::protobuf::internal::MigrationSchema schemas[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  { 0, 7, sizeof(::tensorflow::serving::PredictRequest_InputsEntry_DoNotUse)},
  { 9, -1, sizeof(::tensorflow::serving::PredictRequest)},
  { 17, 24, sizeof(::tensorflow::serving::PredictResponse_OutputsEntry_DoNotUse)},
  { 26, -1, sizeof(::tensorflow::serving::PredictResponse)},
};

static ::google::protobuf::Message const * const file_default_instances[] = {
  reinterpret_cast<const ::google::protobuf::Message*>(&::tensorflow::serving::_PredictRequest_InputsEntry_DoNotUse_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::tensorflow::serving::_PredictRequest_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::tensorflow::serving::_PredictResponse_OutputsEntry_DoNotUse_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::tensorflow::serving::_PredictResponse_default_instance_),
};

::google::protobuf::internal::AssignDescriptorsTable assign_descriptors_table_predict_2eproto = {
  {}, AddDescriptors_predict_2eproto, "predict.proto", schemas,
  file_default_instances, TableStruct_predict_2eproto::offsets,
  file_level_metadata_predict_2eproto, 4, file_level_enum_descriptors_predict_2eproto, file_level_service_descriptors_predict_2eproto,
};

const char descriptor_table_protodef_predict_2eproto[] =
  "\n\rpredict.proto\022\022tensorflow.serving\032\014ten"
  "sor.proto\032\013model.proto\"\342\001\n\016PredictReques"
  "t\0221\n\nmodel_spec\030\001 \001(\0132\035.tensorflow.servi"
  "ng.ModelSpec\022>\n\006inputs\030\002 \003(\0132..tensorflo"
  "w.serving.PredictRequest.InputsEntry\022\025\n\r"
  "output_filter\030\003 \003(\t\032F\n\013InputsEntry\022\013\n\003ke"
  "y\030\001 \001(\t\022&\n\005value\030\002 \001(\0132\027.tensorflow.Tens"
  "orProto:\0028\001\"\320\001\n\017PredictResponse\0221\n\nmodel"
  "_spec\030\002 \001(\0132\035.tensorflow.serving.ModelSp"
  "ec\022A\n\007outputs\030\001 \003(\01320.tensorflow.serving"
  ".PredictResponse.OutputsEntry\032G\n\014Outputs"
  "Entry\022\013\n\003key\030\001 \001(\t\022&\n\005value\030\002 \001(\0132\027.tens"
  "orflow.TensorProto:\0028\001B\003\370\001\001b\006proto3"
  ;
::google::protobuf::internal::DescriptorTable descriptor_table_predict_2eproto = {
  false, InitDefaults_predict_2eproto, 
  descriptor_table_protodef_predict_2eproto,
  "predict.proto", &assign_descriptors_table_predict_2eproto, 515,
};

void AddDescriptors_predict_2eproto() {
  static constexpr ::google::protobuf::internal::InitFunc deps[2] =
  {
    ::AddDescriptors_tensor_2eproto,
    ::AddDescriptors_model_2eproto,
  };
 ::google::protobuf::internal::AddDescriptors(&descriptor_table_predict_2eproto, deps, 2);
}

// Force running AddDescriptors() at dynamic initialization time.
static bool dynamic_init_dummy_predict_2eproto = []() { AddDescriptors_predict_2eproto(); return true; }();
namespace tensorflow {
namespace serving {

// ===================================================================

PredictRequest_InputsEntry_DoNotUse::PredictRequest_InputsEntry_DoNotUse() {}
PredictRequest_InputsEntry_DoNotUse::PredictRequest_InputsEntry_DoNotUse(::google::protobuf::Arena* arena)
    : SuperType(arena) {}
void PredictRequest_InputsEntry_DoNotUse::MergeFrom(const PredictRequest_InputsEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::google::protobuf::Metadata PredictRequest_InputsEntry_DoNotUse::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_predict_2eproto);
  return ::file_level_metadata_predict_2eproto[0];
}
void PredictRequest_InputsEntry_DoNotUse::MergeFrom(
    const ::google::protobuf::Message& other) {
  ::google::protobuf::Message::MergeFrom(other);
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool PredictRequest_InputsEntry_DoNotUse::_ParseMap(const char* begin, const char* end, void* object, ::google::protobuf::internal::ParseContext* ctx) {
  using MF = ::google::protobuf::internal::MapField<
      PredictRequest_InputsEntry_DoNotUse, EntryKeyType, EntryValueType,
      kEntryKeyFieldType, kEntryValueFieldType,
      kEntryDefaultEnumValue>;
  auto mf = static_cast<MF*>(object);
  Parser<MF, ::google::protobuf::Map<EntryKeyType, EntryValueType>> parser(mf);
#define DO_(x) if (!(x)) return false
  DO_(parser.ParseMap(begin, end));
  DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
    parser.key().data(), static_cast<int>(parser.key().length()),
    ::google::protobuf::internal::WireFormatLite::PARSE,
    "tensorflow.serving.PredictRequest.InputsEntry.key"));
#undef DO_
  return true;
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER


// ===================================================================

void PredictRequest::InitAsDefaultInstance() {
  ::tensorflow::serving::_PredictRequest_default_instance_._instance.get_mutable()->model_spec_ = const_cast< ::tensorflow::serving::ModelSpec*>(
      ::tensorflow::serving::ModelSpec::internal_default_instance());
}
class PredictRequest::HasBitSetters {
 public:
  static const ::tensorflow::serving::ModelSpec& model_spec(const PredictRequest* msg);
};

const ::tensorflow::serving::ModelSpec&
PredictRequest::HasBitSetters::model_spec(const PredictRequest* msg) {
  return *msg->model_spec_;
}
void PredictRequest::unsafe_arena_set_allocated_model_spec(
    ::tensorflow::serving::ModelSpec* model_spec) {
  if (GetArenaNoVirtual() == nullptr) {
    delete model_spec_;
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictRequest.model_spec)
}
void PredictRequest::clear_model_spec() {
  if (GetArenaNoVirtual() == nullptr && model_spec_ != nullptr) {
    delete model_spec_;
  }
  model_spec_ = nullptr;
}
void PredictRequest::clear_inputs() {
  inputs_.Clear();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PredictRequest::kModelSpecFieldNumber;
const int PredictRequest::kInputsFieldNumber;
const int PredictRequest::kOutputFilterFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PredictRequest::PredictRequest()
  : ::google::protobuf::Message(), _internal_metadata_(nullptr) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.PredictRequest)
}
PredictRequest::PredictRequest(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena),
  inputs_(arena),
  output_filter_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.PredictRequest)
}
PredictRequest::PredictRequest(const PredictRequest& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(nullptr),
      output_filter_(from.output_filter_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  inputs_.MergeFrom(from.inputs_);
  if (from.has_model_spec()) {
    model_spec_ = new ::tensorflow::serving::ModelSpec(*from.model_spec_);
  } else {
    model_spec_ = nullptr;
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.PredictRequest)
}

void PredictRequest::SharedCtor() {
  ::google::protobuf::internal::InitSCC(
      &scc_info_PredictRequest_predict_2eproto.base);
  model_spec_ = nullptr;
}

PredictRequest::~PredictRequest() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.PredictRequest)
  SharedDtor();
}

void PredictRequest::SharedDtor() {
  GOOGLE_DCHECK(GetArenaNoVirtual() == nullptr);
  if (this != internal_default_instance()) delete model_spec_;
}

void PredictRequest::ArenaDtor(void* object) {
  PredictRequest* _this = reinterpret_cast< PredictRequest* >(object);
  (void)_this;
}
void PredictRequest::RegisterArenaDtor(::google::protobuf::Arena*) {
}
void PredictRequest::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const PredictRequest& PredictRequest::default_instance() {
  ::google::protobuf::internal::InitSCC(&::scc_info_PredictRequest_predict_2eproto.base);
  return *internal_default_instance();
}


void PredictRequest::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.PredictRequest)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  inputs_.Clear();
  output_filter_.Clear();
  if (GetArenaNoVirtual() == nullptr && model_spec_ != nullptr) {
    delete model_spec_;
  }
  model_spec_ = nullptr;
  _internal_metadata_.Clear();
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
const char* PredictRequest::_InternalParse(const char* begin, const char* end, void* object,
                  ::google::protobuf::internal::ParseContext* ctx) {
  auto msg = static_cast<PredictRequest*>(object);
  ::google::protobuf::int32 size; (void)size;
  int depth; (void)depth;
  ::google::protobuf::uint32 tag;
  ::google::protobuf::internal::ParseFunc parser_till_end; (void)parser_till_end;
  auto ptr = begin;
  while (ptr < end) {
    ptr = ::google::protobuf::io::Parse32(ptr, &tag);
    GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
    switch (tag >> 3) {
      // .tensorflow.serving.ModelSpec model_spec = 1;
      case 1: {
        if (static_cast<::google::protobuf::uint8>(tag) != 10) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        parser_till_end = ::tensorflow::serving::ModelSpec::_InternalParse;
        object = msg->mutable_model_spec();
        if (size > end - ptr) goto len_delim_till_end;
        ptr += size;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ctx->ParseExactRange(
            {parser_till_end, object}, ptr - size, ptr));
        break;
      }
      // map<string, .tensorflow.TensorProto> inputs = 2;
      case 2: {
        if (static_cast<::google::protobuf::uint8>(tag) != 18) goto handle_unusual;
        do {
          ptr = ::google::protobuf::io::ReadSize(ptr, &size);
          GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
          parser_till_end = ::google::protobuf::internal::SlowMapEntryParser;
          auto parse_map = ::tensorflow::serving::PredictRequest_InputsEntry_DoNotUse::_ParseMap;
          ctx->extra_parse_data().payload.clear();
          ctx->extra_parse_data().parse_map = parse_map;
          object = &msg->inputs_;
          if (size > end - ptr) goto len_delim_till_end;
          auto newend = ptr + size;
          GOOGLE_PROTOBUF_PARSER_ASSERT(parse_map(ptr, newend, object, ctx));
          ptr = newend;
          if (ptr >= end) break;
        } while ((::google::protobuf::io::UnalignedLoad<::google::protobuf::uint64>(ptr) & 255) == 18 && (ptr += 1));
        break;
      }
      // repeated string output_filter = 3;
      case 3: {
        if (static_cast<::google::protobuf::uint8>(tag) != 26) goto handle_unusual;
        do {
          ptr = ::google::protobuf::io::ReadSize(ptr, &size);
          GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
          ctx->extra_parse_data().SetFieldName("tensorflow.serving.PredictRequest.output_filter");
          object = msg->add_output_filter();
          if (size > end - ptr + ::google::protobuf::internal::ParseContext::kSlopBytes) {
            parser_till_end = ::google::protobuf::internal::GreedyStringParserUTF8;
            goto string_till_end;
          }
          GOOGLE_PROTOBUF_PARSER_ASSERT(::google::protobuf::internal::StringCheckUTF8(ptr, size, ctx));
          ::google::protobuf::internal::InlineGreedyStringParser(object, ptr, size, ctx);
          ptr += size;
          if (ptr >= end) break;
        } while ((::google::protobuf::io::UnalignedLoad<::google::protobuf::uint64>(ptr) & 255) == 26 && (ptr += 1));
        break;
      }
      default: {
      handle_unusual:
        if ((tag & 7) == 4 || tag == 0) {
          ctx->EndGroup(tag);
          return ptr;
        }
        auto res = UnknownFieldParse(tag, {_InternalParse, msg},
          ptr, end, msg->_internal_metadata_.mutable_unknown_fields(), ctx);
        ptr = res.first;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr != nullptr);
        if (res.second) return ptr;
      }
    }  // switch
  }  // while
  return ptr;
string_till_end:
  static_cast<::std::string*>(object)->clear();
  static_cast<::std::string*>(object)->reserve(size);
  goto len_delim_till_end;
len_delim_till_end:
  return ctx->StoreAndTailCall(ptr, end, {_InternalParse, msg},
                               {parser_till_end, object}, size);
}
#else  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool PredictRequest::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!PROTOBUF_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.PredictRequest)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .tensorflow.serving.ModelSpec model_spec = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (10 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_model_spec()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // map<string, .tensorflow.TensorProto> inputs = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (18 & 0xFF)) {
          PredictRequest_InputsEntry_DoNotUse::Parser< ::google::protobuf::internal::MapField<
              PredictRequest_InputsEntry_DoNotUse,
              ::std::string, ::tensorflow::TensorProto,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
              0 >,
            ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto > > parser(&inputs_);
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
              input, &parser));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.key().data(), static_cast<int>(parser.key().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "tensorflow.serving.PredictRequest.InputsEntry.key"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated string output_filter = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (26 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_output_filter()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->output_filter(this->output_filter_size() - 1).data(),
            static_cast<int>(this->output_filter(this->output_filter_size() - 1).length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "tensorflow.serving.PredictRequest.output_filter"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.PredictRequest)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.PredictRequest)
  return false;
#undef DO_
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER

void PredictRequest::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.PredictRequest)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, HasBitSetters::model_spec(this), output);
  }

  // map<string, .tensorflow.TensorProto> inputs = 2;
  if (!this->inputs().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "tensorflow.serving.PredictRequest.InputsEntry.key");
      }
    };

    if (output->IsSerializationDeterministic() &&
        this->inputs().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->inputs().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->inputs().begin();
          it != this->inputs().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      ::std::unique_ptr<PredictRequest_InputsEntry_DoNotUse> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(inputs_.NewEntryWrapper(items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(2, *entry, output);
        if (entry->GetArena() != nullptr) {
          entry.release();
        }
        Utf8Check::Check(&(*items[static_cast<ptrdiff_t>(i)]));
      }
    } else {
      ::std::unique_ptr<PredictRequest_InputsEntry_DoNotUse> entry;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->inputs().begin();
          it != this->inputs().end(); ++it) {
        entry.reset(inputs_.NewEntryWrapper(it->first, it->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(2, *entry, output);
        if (entry->GetArena() != nullptr) {
          entry.release();
        }
        Utf8Check::Check(&(*it));
      }
    }
  }

  // repeated string output_filter = 3;
  for (int i = 0, n = this->output_filter_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->output_filter(i).data(), static_cast<int>(this->output_filter(i).length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "tensorflow.serving.PredictRequest.output_filter");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      3, this->output_filter(i), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:tensorflow.serving.PredictRequest)
}

::google::protobuf::uint8* PredictRequest::InternalSerializeWithCachedSizesToArray(
    ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.PredictRequest)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, HasBitSetters::model_spec(this), target);
  }

  // map<string, .tensorflow.TensorProto> inputs = 2;
  if (!this->inputs().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "tensorflow.serving.PredictRequest.InputsEntry.key");
      }
    };

    if (false &&
        this->inputs().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->inputs().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->inputs().begin();
          it != this->inputs().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      ::std::unique_ptr<PredictRequest_InputsEntry_DoNotUse> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(inputs_.NewEntryWrapper(items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second));
        target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessageNoVirtualToArray(2, *entry, target);
        if (entry->GetArena() != nullptr) {
          entry.release();
        }
        Utf8Check::Check(&(*items[static_cast<ptrdiff_t>(i)]));
      }
    } else {
      ::std::unique_ptr<PredictRequest_InputsEntry_DoNotUse> entry;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->inputs().begin();
          it != this->inputs().end(); ++it) {
        entry.reset(inputs_.NewEntryWrapper(it->first, it->second));
        target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessageNoVirtualToArray(2, *entry, target);
        if (entry->GetArena() != nullptr) {
          entry.release();
        }
        Utf8Check::Check(&(*it));
      }
    }
  }

  // repeated string output_filter = 3;
  for (int i = 0, n = this->output_filter_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->output_filter(i).data(), static_cast<int>(this->output_filter(i).length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "tensorflow.serving.PredictRequest.output_filter");
    target = ::google::protobuf::internal::WireFormatLite::
      WriteStringToArray(3, this->output_filter(i), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.PredictRequest)
  return target;
}

size_t PredictRequest::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.PredictRequest)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // map<string, .tensorflow.TensorProto> inputs = 2;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->inputs_size());
  {
    ::std::unique_ptr<PredictRequest_InputsEntry_DoNotUse> entry;
    for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
        it = this->inputs().begin();
        it != this->inputs().end(); ++it) {
      if (entry.get() != nullptr && entry->GetArena() != nullptr) {
        entry.release();
      }
      entry.reset(inputs_.NewEntryWrapper(it->first, it->second));
      total_size += ::google::protobuf::internal::WireFormatLite::
          MessageSizeNoVirtual(*entry);
    }
    if (entry.get() != nullptr && entry->GetArena() != nullptr) {
      entry.release();
    }
  }

  // repeated string output_filter = 3;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->output_filter_size());
  for (int i = 0, n = this->output_filter_size(); i < n; i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->output_filter(i));
  }

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *model_spec_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void PredictRequest::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.PredictRequest)
  GOOGLE_DCHECK_NE(&from, this);
  const PredictRequest* source =
      ::google::protobuf::DynamicCastToGenerated<PredictRequest>(
          &from);
  if (source == nullptr) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.PredictRequest)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.PredictRequest)
    MergeFrom(*source);
  }
}

void PredictRequest::MergeFrom(const PredictRequest& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.PredictRequest)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  inputs_.MergeFrom(from.inputs_);
  output_filter_.MergeFrom(from.output_filter_);
  if (from.has_model_spec()) {
    mutable_model_spec()->::tensorflow::serving::ModelSpec::MergeFrom(from.model_spec());
  }
}

void PredictRequest::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.PredictRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void PredictRequest::CopyFrom(const PredictRequest& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.PredictRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PredictRequest::IsInitialized() const {
  return true;
}

void PredictRequest::Swap(PredictRequest* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    PredictRequest* temp = New(GetArenaNoVirtual());
    temp->MergeFrom(*other);
    other->CopyFrom(*this);
    InternalSwap(temp);
    if (GetArenaNoVirtual() == nullptr) {
      delete temp;
    }
  }
}
void PredictRequest::UnsafeArenaSwap(PredictRequest* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void PredictRequest::InternalSwap(PredictRequest* other) {
  using std::swap;
  _internal_metadata_.Swap(&other->_internal_metadata_);
  inputs_.Swap(&other->inputs_);
  output_filter_.InternalSwap(CastToBase(&other->output_filter_));
  swap(model_spec_, other->model_spec_);
}

::google::protobuf::Metadata PredictRequest::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_predict_2eproto);
  return ::file_level_metadata_predict_2eproto[kIndexInFileMessages];
}


// ===================================================================

PredictResponse_OutputsEntry_DoNotUse::PredictResponse_OutputsEntry_DoNotUse() {}
PredictResponse_OutputsEntry_DoNotUse::PredictResponse_OutputsEntry_DoNotUse(::google::protobuf::Arena* arena)
    : SuperType(arena) {}
void PredictResponse_OutputsEntry_DoNotUse::MergeFrom(const PredictResponse_OutputsEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::google::protobuf::Metadata PredictResponse_OutputsEntry_DoNotUse::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_predict_2eproto);
  return ::file_level_metadata_predict_2eproto[2];
}
void PredictResponse_OutputsEntry_DoNotUse::MergeFrom(
    const ::google::protobuf::Message& other) {
  ::google::protobuf::Message::MergeFrom(other);
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool PredictResponse_OutputsEntry_DoNotUse::_ParseMap(const char* begin, const char* end, void* object, ::google::protobuf::internal::ParseContext* ctx) {
  using MF = ::google::protobuf::internal::MapField<
      PredictResponse_OutputsEntry_DoNotUse, EntryKeyType, EntryValueType,
      kEntryKeyFieldType, kEntryValueFieldType,
      kEntryDefaultEnumValue>;
  auto mf = static_cast<MF*>(object);
  Parser<MF, ::google::protobuf::Map<EntryKeyType, EntryValueType>> parser(mf);
#define DO_(x) if (!(x)) return false
  DO_(parser.ParseMap(begin, end));
  DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
    parser.key().data(), static_cast<int>(parser.key().length()),
    ::google::protobuf::internal::WireFormatLite::PARSE,
    "tensorflow.serving.PredictResponse.OutputsEntry.key"));
#undef DO_
  return true;
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER


// ===================================================================

void PredictResponse::InitAsDefaultInstance() {
  ::tensorflow::serving::_PredictResponse_default_instance_._instance.get_mutable()->model_spec_ = const_cast< ::tensorflow::serving::ModelSpec*>(
      ::tensorflow::serving::ModelSpec::internal_default_instance());
}
class PredictResponse::HasBitSetters {
 public:
  static const ::tensorflow::serving::ModelSpec& model_spec(const PredictResponse* msg);
};

const ::tensorflow::serving::ModelSpec&
PredictResponse::HasBitSetters::model_spec(const PredictResponse* msg) {
  return *msg->model_spec_;
}
void PredictResponse::unsafe_arena_set_allocated_model_spec(
    ::tensorflow::serving::ModelSpec* model_spec) {
  if (GetArenaNoVirtual() == nullptr) {
    delete model_spec_;
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictResponse.model_spec)
}
void PredictResponse::clear_model_spec() {
  if (GetArenaNoVirtual() == nullptr && model_spec_ != nullptr) {
    delete model_spec_;
  }
  model_spec_ = nullptr;
}
void PredictResponse::clear_outputs() {
  outputs_.Clear();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PredictResponse::kModelSpecFieldNumber;
const int PredictResponse::kOutputsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PredictResponse::PredictResponse()
  : ::google::protobuf::Message(), _internal_metadata_(nullptr) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.PredictResponse)
}
PredictResponse::PredictResponse(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena),
  outputs_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.PredictResponse)
}
PredictResponse::PredictResponse(const PredictResponse& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(nullptr) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  outputs_.MergeFrom(from.outputs_);
  if (from.has_model_spec()) {
    model_spec_ = new ::tensorflow::serving::ModelSpec(*from.model_spec_);
  } else {
    model_spec_ = nullptr;
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.PredictResponse)
}

void PredictResponse::SharedCtor() {
  ::google::protobuf::internal::InitSCC(
      &scc_info_PredictResponse_predict_2eproto.base);
  model_spec_ = nullptr;
}

PredictResponse::~PredictResponse() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.PredictResponse)
  SharedDtor();
}

void PredictResponse::SharedDtor() {
  GOOGLE_DCHECK(GetArenaNoVirtual() == nullptr);
  if (this != internal_default_instance()) delete model_spec_;
}

void PredictResponse::ArenaDtor(void* object) {
  PredictResponse* _this = reinterpret_cast< PredictResponse* >(object);
  (void)_this;
}
void PredictResponse::RegisterArenaDtor(::google::protobuf::Arena*) {
}
void PredictResponse::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const PredictResponse& PredictResponse::default_instance() {
  ::google::protobuf::internal::InitSCC(&::scc_info_PredictResponse_predict_2eproto.base);
  return *internal_default_instance();
}


void PredictResponse::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.PredictResponse)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  outputs_.Clear();
  if (GetArenaNoVirtual() == nullptr && model_spec_ != nullptr) {
    delete model_spec_;
  }
  model_spec_ = nullptr;
  _internal_metadata_.Clear();
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
const char* PredictResponse::_InternalParse(const char* begin, const char* end, void* object,
                  ::google::protobuf::internal::ParseContext* ctx) {
  auto msg = static_cast<PredictResponse*>(object);
  ::google::protobuf::int32 size; (void)size;
  int depth; (void)depth;
  ::google::protobuf::uint32 tag;
  ::google::protobuf::internal::ParseFunc parser_till_end; (void)parser_till_end;
  auto ptr = begin;
  while (ptr < end) {
    ptr = ::google::protobuf::io::Parse32(ptr, &tag);
    GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
    switch (tag >> 3) {
      // map<string, .tensorflow.TensorProto> outputs = 1;
      case 1: {
        if (static_cast<::google::protobuf::uint8>(tag) != 10) goto handle_unusual;
        do {
          ptr = ::google::protobuf::io::ReadSize(ptr, &size);
          GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
          parser_till_end = ::google::protobuf::internal::SlowMapEntryParser;
          auto parse_map = ::tensorflow::serving::PredictResponse_OutputsEntry_DoNotUse::_ParseMap;
          ctx->extra_parse_data().payload.clear();
          ctx->extra_parse_data().parse_map = parse_map;
          object = &msg->outputs_;
          if (size > end - ptr) goto len_delim_till_end;
          auto newend = ptr + size;
          GOOGLE_PROTOBUF_PARSER_ASSERT(parse_map(ptr, newend, object, ctx));
          ptr = newend;
          if (ptr >= end) break;
        } while ((::google::protobuf::io::UnalignedLoad<::google::protobuf::uint64>(ptr) & 255) == 10 && (ptr += 1));
        break;
      }
      // .tensorflow.serving.ModelSpec model_spec = 2;
      case 2: {
        if (static_cast<::google::protobuf::uint8>(tag) != 18) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        parser_till_end = ::tensorflow::serving::ModelSpec::_InternalParse;
        object = msg->mutable_model_spec();
        if (size > end - ptr) goto len_delim_till_end;
        ptr += size;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ctx->ParseExactRange(
            {parser_till_end, object}, ptr - size, ptr));
        break;
      }
      default: {
      handle_unusual:
        if ((tag & 7) == 4 || tag == 0) {
          ctx->EndGroup(tag);
          return ptr;
        }
        auto res = UnknownFieldParse(tag, {_InternalParse, msg},
          ptr, end, msg->_internal_metadata_.mutable_unknown_fields(), ctx);
        ptr = res.first;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr != nullptr);
        if (res.second) return ptr;
      }
    }  // switch
  }  // while
  return ptr;
len_delim_till_end:
  return ctx->StoreAndTailCall(ptr, end, {_InternalParse, msg},
                               {parser_till_end, object}, size);
}
#else  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool PredictResponse::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!PROTOBUF_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.PredictResponse)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // map<string, .tensorflow.TensorProto> outputs = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (10 & 0xFF)) {
          PredictResponse_OutputsEntry_DoNotUse::Parser< ::google::protobuf::internal::MapField<
              PredictResponse_OutputsEntry_DoNotUse,
              ::std::string, ::tensorflow::TensorProto,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
              0 >,
            ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto > > parser(&outputs_);
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
              input, &parser));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.key().data(), static_cast<int>(parser.key().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "tensorflow.serving.PredictResponse.OutputsEntry.key"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .tensorflow.serving.ModelSpec model_spec = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (18 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_model_spec()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.PredictResponse)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.PredictResponse)
  return false;
#undef DO_
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER

void PredictResponse::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.PredictResponse)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // map<string, .tensorflow.TensorProto> outputs = 1;
  if (!this->outputs().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "tensorflow.serving.PredictResponse.OutputsEntry.key");
      }
    };

    if (output->IsSerializationDeterministic() &&
        this->outputs().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->outputs().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->outputs().begin();
          it != this->outputs().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      ::std::unique_ptr<PredictResponse_OutputsEntry_DoNotUse> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(outputs_.NewEntryWrapper(items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(1, *entry, output);
        if (entry->GetArena() != nullptr) {
          entry.release();
        }
        Utf8Check::Check(&(*items[static_cast<ptrdiff_t>(i)]));
      }
    } else {
      ::std::unique_ptr<PredictResponse_OutputsEntry_DoNotUse> entry;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->outputs().begin();
          it != this->outputs().end(); ++it) {
        entry.reset(outputs_.NewEntryWrapper(it->first, it->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(1, *entry, output);
        if (entry->GetArena() != nullptr) {
          entry.release();
        }
        Utf8Check::Check(&(*it));
      }
    }
  }

  // .tensorflow.serving.ModelSpec model_spec = 2;
  if (this->has_model_spec()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, HasBitSetters::model_spec(this), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:tensorflow.serving.PredictResponse)
}

::google::protobuf::uint8* PredictResponse::InternalSerializeWithCachedSizesToArray(
    ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.PredictResponse)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // map<string, .tensorflow.TensorProto> outputs = 1;
  if (!this->outputs().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "tensorflow.serving.PredictResponse.OutputsEntry.key");
      }
    };

    if (false &&
        this->outputs().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->outputs().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->outputs().begin();
          it != this->outputs().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      ::std::unique_ptr<PredictResponse_OutputsEntry_DoNotUse> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(outputs_.NewEntryWrapper(items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second));
        target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessageNoVirtualToArray(1, *entry, target);
        if (entry->GetArena() != nullptr) {
          entry.release();
        }
        Utf8Check::Check(&(*items[static_cast<ptrdiff_t>(i)]));
      }
    } else {
      ::std::unique_ptr<PredictResponse_OutputsEntry_DoNotUse> entry;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->outputs().begin();
          it != this->outputs().end(); ++it) {
        entry.reset(outputs_.NewEntryWrapper(it->first, it->second));
        target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessageNoVirtualToArray(1, *entry, target);
        if (entry->GetArena() != nullptr) {
          entry.release();
        }
        Utf8Check::Check(&(*it));
      }
    }
  }

  // .tensorflow.serving.ModelSpec model_spec = 2;
  if (this->has_model_spec()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        2, HasBitSetters::model_spec(this), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.PredictResponse)
  return target;
}

size_t PredictResponse::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.PredictResponse)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // map<string, .tensorflow.TensorProto> outputs = 1;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->outputs_size());
  {
    ::std::unique_ptr<PredictResponse_OutputsEntry_DoNotUse> entry;
    for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
        it = this->outputs().begin();
        it != this->outputs().end(); ++it) {
      if (entry.get() != nullptr && entry->GetArena() != nullptr) {
        entry.release();
      }
      entry.reset(outputs_.NewEntryWrapper(it->first, it->second));
      total_size += ::google::protobuf::internal::WireFormatLite::
          MessageSizeNoVirtual(*entry);
    }
    if (entry.get() != nullptr && entry->GetArena() != nullptr) {
      entry.release();
    }
  }

  // .tensorflow.serving.ModelSpec model_spec = 2;
  if (this->has_model_spec()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *model_spec_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void PredictResponse::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.PredictResponse)
  GOOGLE_DCHECK_NE(&from, this);
  const PredictResponse* source =
      ::google::protobuf::DynamicCastToGenerated<PredictResponse>(
          &from);
  if (source == nullptr) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.PredictResponse)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.PredictResponse)
    MergeFrom(*source);
  }
}

void PredictResponse::MergeFrom(const PredictResponse& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.PredictResponse)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  outputs_.MergeFrom(from.outputs_);
  if (from.has_model_spec()) {
    mutable_model_spec()->::tensorflow::serving::ModelSpec::MergeFrom(from.model_spec());
  }
}

void PredictResponse::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.PredictResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void PredictResponse::CopyFrom(const PredictResponse& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.PredictResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PredictResponse::IsInitialized() const {
  return true;
}

void PredictResponse::Swap(PredictResponse* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    PredictResponse* temp = New(GetArenaNoVirtual());
    temp->MergeFrom(*other);
    other->CopyFrom(*this);
    InternalSwap(temp);
    if (GetArenaNoVirtual() == nullptr) {
      delete temp;
    }
  }
}
void PredictResponse::UnsafeArenaSwap(PredictResponse* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void PredictResponse::InternalSwap(PredictResponse* other) {
  using std::swap;
  _internal_metadata_.Swap(&other->_internal_metadata_);
  outputs_.Swap(&other->outputs_);
  swap(model_spec_, other->model_spec_);
}

::google::protobuf::Metadata PredictResponse::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_predict_2eproto);
  return ::file_level_metadata_predict_2eproto[kIndexInFileMessages];
}


// @@protoc_insertion_point(namespace_scope)
}  // namespace serving
}  // namespace tensorflow
namespace google {
namespace protobuf {
template<> PROTOBUF_NOINLINE ::tensorflow::serving::PredictRequest_InputsEntry_DoNotUse* Arena::CreateMaybeMessage< ::tensorflow::serving::PredictRequest_InputsEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::PredictRequest_InputsEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::serving::PredictRequest* Arena::CreateMaybeMessage< ::tensorflow::serving::PredictRequest >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::PredictRequest >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::serving::PredictResponse_OutputsEntry_DoNotUse* Arena::CreateMaybeMessage< ::tensorflow::serving::PredictResponse_OutputsEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::PredictResponse_OutputsEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::serving::PredictResponse* Arena::CreateMaybeMessage< ::tensorflow::serving::PredictResponse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::PredictResponse >(arena);
}
}  // namespace protobuf
}  // namespace google

// @@protoc_insertion_point(global_scope)
#include <google/protobuf/port_undef.inc>
